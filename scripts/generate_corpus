#!/usr/bin/env python2.7
# -*- coding: utf-8; -*-

##################################################################
# Libraries
import re
import random
import sys
import xml.etree.ElementTree as ET

from datetime import datetime
from alt_argparse import argparser
from alt_fio import AltFileInput, AltFileOutput
from ld.stringtools import TIMEFMT, str2time

##################################################################
# Constants and Variables
TABSEP  = re.compile(r'\t')
BARSEP  = re.compile(r"^([^|]+)\|")
COMASEP = re.compile(r',')
WORDSEP = re.compile(r"[-\]\s:;,.?!(){}[]+")
SPACES  = re.compile(r"\s+")
EMO_RE  = re.compile(r"[-:][])([*]")
UNINF_WORD   = re.compile(r"(?:[@#]|www|http)")
INF_WORD     = re.compile(r"^[\w][\w']+[\w.']$")
INF_WORD_SET = set(["ich", "meiner", "mir", "mich", "du", "deiner", "dir",
                    "dich", "er", "seiner", "ihm", "ihn", "sie", "ihrer",
                    "ihr", "eurer", "ihnen", "meine", "meinem", "meinen",
                    "deine", "deinem", "deinen", "seine", "seinem", "seinen",
                    "ihre", "ihrem", "ihren", "eure", "eurem", "euren", "euch",
                    "sich", "die", "der", "dem", "den", "das", "mit", "nach",
                    "eine", "einem", "einen", "ein", "keine", "keinem",
                    "keinen", "kein", "aus", "zu", "zum", "zur", "von", "vom",
                    "bei", "beim", "seit", "ausser", u"außer", "entgegen",
                    u"gegenüber", "wenn", "weil", "denn", "an", "ans", "am",
                    "in", "im", "vor", "vorm", "vors", u"über", u"überm",
                    u"übers", "unter", "unterm", "unters", "hinter", "hinterm",
                    "hinters", "neben", "zwischen", "ohne", "durch", "durchs",
                    "um", "sollte", "soll", u"müssen", "musste", "sollst",
                    "musst", "mussten", "musstest", u"müsst", "muss", "dürfen",
                    "darf", "darsft", "dürft", u"durfte", "durftest", "durften",
                    "sollt", "sollen", "mag", "sollten", "kannst",
                    "könnt", "könntet", "solltet", "solltest", "sollten",
                    u"können", "konnten", "konntest", "konntet", "konnte",
                    "konnte", "dessen", "deren", "kaum", "nicht", "nichts",
                    "es", "man", "hast", "hat", "haben", "habt", "hatte",
                    "hattet", "hattest", "hatten", "ist", "sind", "seid",
                    "war", "warst", "wart", "wie", "als", "dass", "ob", "sehr",
                    "ziemlich", "und", "oder", "aber", "sowie", "auch", "wieder",
                    "wiederum", "nur", "viel", "wenig", "recht"])

LANG    = "de"
DEFAULTN = 333

# by default classify lang will be a dummy function, which will always return
# true
classify_lang = lambda x: True
emoset       = set([])
emointersect = set([])
msg_id   = ""
msg      = ""
msg_time = min_time = max_time = None
words    = []
fields   = []

# lists of correspondingly classified tweets
emowl  = []
emotl  = []
reml   = []
trgl   = None        # address of list to which given tweet should be added
# all available groups for which we should collect samples
groups  = {"emotional words": emowl, \
               "emoticons": emotl, \
               "random": reml}
samplen = []

##################################################################
# Methods
def read_emo_list(ifile):
    """Read a list of emotionally tagged words from file."""
    words = []
    mobj  = None
    wset  = set([])
    for line in AltFileInput(ifile):
        line  = line.lower()
        words = TABSEP.split(line)
        mobj  = BARSEP.match(words[0])
         # add the lemma word and possible grammar forms
        wset.update([((mobj and mobj.group(1)) or words[0])] + \
            ((len(words) == 3 and COMASEP.split(words[2])) or []))
    ifile.close()
    return wset

def normalize_space(istr):
    """Squeze repeating and strip leading and trailing spaces."""
    return SPACES.sub(' ', istr.strip())

def filter_word(word):
    """Check if word is informative or not."""
    return (not UNINF_WORD.match(word)) and \
        INF_WORD.match(word)

def is_good(words):
    """Check if message is informative enoughto be included into corpus."""
    flt_wrds = filter(filter_word, words)
    return classify_lang(' '.join(flt_wrds)) and len(flt_wrds) > 4 and \
        INF_WORD_SET.intersection(flt_wrds)

def normalizen(samplen, groupsn):
    """Re-distribute required number of samples if some group can't provide
    enough examples."""
    lsample = len(samplen)
    assert(lsample == len(groupsn))
    deltas = [samplen[i] - groupsn[i] for i in xrange(lsample)]
    pdeltas = []; posd = 0
    ndeltas = []; negd = 0
    zeros   = []
    for (i, d) in enumerate(deltas):
        if d < 0:
            ndeltas.append((i, d))
            negd += d
        elif d > 0:
            pdeltas.append((i, d))
            posd += d
        else:
            zeros.append((i, d))
    # if none of required samples exceeds the len of its corresponding group
    # return this vector of samples
    if not pdeltas:
        return samplen
    # if all the groups are already exhaustively used, check if none of the
    # number of required samples exceeds the actual number of available
    # sentences and abate appetite if it does.
    elif not negd:
        for (i, d) in pdeltas:
            samplen[i] -= d
        return samplen
    else:
        # decrease the number of required samples for groups which are
        # underrepresented
        for (i, d) in pdeltas:
            samplen[i] -= d
        # lnd - is the number of groups from which we could get new samples of
        # gone ones
        lnd = len(ndeltas)
        # determine an average number of samples which could be obtained from
        # each of excessively represented group and the remainder of samples
        # are left over after distribution
        avg, rem = divmod(posd, lnd)
        for (i, d) in ndeltas:
            samplen[i] += avg     # since d is negative, subtracting it will
                                  # increase samplen i
            if rem:
                samplen[i] += 1
                rem -= 1
        return normalizen(samplen, groupsn)

##################################################################
# Arguments
argparser.description="""Utility for randomly choosing lines from a corpus
based on predefined criteria."""
argparser.add_file_argument("-m", "--emo-file", help = """file with a list of
 emotional expressions in SentiWS format""", required = True)
argparser.add_argument("-n", "--number-of-samples", help = """max number of samples
which should be collected for each of {:d} groups (if some group does not provide enough
samples, N for other excessively represented groups will be correspondingly
increased)""".format(len(groups)), type = int, default = DEFAULTN)
# the following option is currently switched off, since langid is not installed
# on all servers
argparser.add_argument("-l", "--lang", help = """identify language of messages which
 currently should be {}""".format(LANG), default = LANG)
argparser.add_argument("-c", "--corpus-name", help = """name of subcorpus to be created""", \
                       type = str, default = "UNKNOWN")
args = argparser.parse_args()

if args.lang:
    import langid
    # import guess_language
    lang = args.lang.lower()
    classify_lang = lambda x: langid.classify(x)[0] == lang

emoset  = read_emo_list(args.emo_file)
foutput = AltFileOutput(flush = args.flush)
finput  = AltFileInput(*args.files, skip_line = args.skip_line, \
                            print_func = foutput.fprint)

##################################################################
# Main
for line in finput:
    fields   = TABSEP.split(line)
    msg_time = str2time(fields[-2])
    msg      = fields[-1]
    words    = WORDSEP.split(msg.lower())
    if not is_good(words):
        continue
    emointersect = emoset.intersection(set(words))
    # one group is messages which contain at least one emotionally marked word
    if emointersect:
        trgl = emowl
        # another group has at least one emoticon
    elif EMO_RE.search(msg):
        trgl = emotl
        # if a message contains at least 5 words add it to residual group
    else:
        trgl = reml
    # update minimal and maximal time if needed
    if not min_time or min_time > msg_time:
        min_time = msg_time
    if not max_time or max_time < msg_time:
        max_time = msg_time
    # add tweet to dedicated list
    trgl.append(fields)

# now, decide what number of samples should be gathered for each group

# first assume that all samples are distributed evenly
samplen = [args.number_of_samples] * len(groups)
# then check, if some of the groups can't provide enough samples, and
# re-distribute this free weight to other groups if they can yield additional
# cases
samplen = normalizen(samplen, [len(g) for k, g in groups.iteritems()])

# generate an XML tree
root = ET.Element("subcorpus")
root.set("name", args.corpus_name)
root.set("corpus-creation-time", datetime.utcnow().strftime(TIMEFMT))
root.set("gather-start-time", min_time.strftime(TIMEFMT))
root.set("gather-end-time", max_time.strftime(TIMEFMT))

tweet = None
subtree = None
for ((groupname, group), n) in zip(groups.iteritems(), samplen):
    print >> sys.stderr, "Generating {:d} samples for group {:s}".format(n, \
                                                                         groupname)
    subtree = ET.SubElement(root, "subsubcorpus", {"type": groupname})
    for (msg_id, msg_time, msg) in random.sample(group, n):
        tweet = ET.SubElement(subtree, "tweet", \
                              {"id": msg_id, "time": msg_time})
        tweet.text = normalize_space(msg)
print ET.tostring(root)
