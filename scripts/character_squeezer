#!/usr/bin/env python2.7
# -*- coding: utf-8; -*-

"""
Script for squeezing multiply repeated characters.

"""

##################################################################
# Libraries
from tokenizer import EOS_TAG
from ld import character_squeezer, UNIGRAM_DEFAULT_FILE, \
    BIGRAM_DEFAULT_FILE
from alt_ngram import BOL, EOL
from alt_argparse import argparser
from alt_fio import AltFileInput, AltFileOutput

import os
import re
import sys
import pickle
from collections import deque

##################################################################
# Constants
# maximum number of candidates to be generated
MAX_CANDIDATES = -1

##################################################################
# Variables
tokens = []
left_word = BOL
right_word = EOL
elong_word = ''
correct_word = ''

##################################################################
# Methods
def __print_squeezed_token__():
    """Squeeze elongated word and append it to token list if needed."""
    global left_word, right_word, elong_word
    if not elong_word:
        return None
    correct_word = char_squeezer.squeeze_characters(elong_word, left_word, \
                                                        right_word)
    elong_word = ''
    foutput.fprint(correct_word)
    left_word = correct_word

##################################################################
# Processing Arguments
# note: some options are already set up by alt_argparser
argparser.description="""Utility for restoring original form of words
with deliberately multiply repeated letters."""
argparser.add_argument("-c", "--esc-char", help = """escape character which should
precede lines with meta-information""", nargs = 1, type = str, \
                           default = os.environ.get("SOCMEDIA_ESC_CHAR", ""))
argparser.add_argument("-d", "--dictionary", help = """reference dictionary for
checking words""", default = "de_CH")
argparser.add_argument("-b", "--bigram-prob-file", help="file with bigram probabilities", \
                           default = BIGRAM_DEFAULT_FILE)
argparser.add_argument("-u", "--unigram-prob-file", help="file with unigram probabilities", \
                           default = UNIGRAM_DEFAULT_FILE)
argparser.add_argument("-p", "--prob-file", help = """additional file with
statistics about usage of certain prolongated form""", \
                           default = character_squeezer.DEFAULT_LENGTH_PROB)
argparser.add_argument("-1", "--single-candidate", help = """perform the
replacement with one of the generated candidates in place instead of outputting
a list of all possible candidates with their probabilities (the default)""", \
                           action = "store_true")
args = argparser.parse_args()

##################################################################
# Main
skip_line = args.skip_line
# probabilities of replacements
esc_char = args.esc_char
skip_line = args.skip_line

foutput = AltFileOutput(encoding = args.encoding, flush = args.flush)
finput = AltFileInput(*args.files, print_func = foutput.fprint)

char_squeezer = character_squeezer.CharacterSqueezer(True, args.prob_file, \
                                                         args.unigram_prob_file, \
                                                         args.bigram_prob_file, \
                                                         args.dictionary)
# iterate over input lines
for line in finput:
    # print empty and skip lines unchanged
    if line == skip_line or not line:
        left_word = BOL
        foutput.fprint(line)
    # output all lines with meta information except lines about replacements
    elif line[0] == esc_char:
        foutput.fprint(line)
    # if line is not empty and does not hold any meta information, it
    # represents a regular token which should be processed
    else:
        if line == EOS_TAG:
            right_word == EOL
            # if elongated word was present, correct it and print
            __print_squeezed_token__()
            foutput.fprint(line)
            left_word == BOL
        else:
            right_word == line
            __print_squeezed_token__()
            if char_squeezer.has_repetition(line):
                elong_word = line
            else:
                foutput.fprint(line)
                left_word = line
right_word = EOL
__print_squeezed_token__()

