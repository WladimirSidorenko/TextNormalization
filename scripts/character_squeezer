#!/usr/bin/env python2.7
# -*- coding: utf-8; -*-

##################################################################
# TODO:

##################################################################
# Libraries
import re
import sys

from alt_argparse import argparser
from alt_fio import AltFileInput, AltFileOutput
from ld import repeated_chars
from ld.stringtools import adjust_case
from alt_hunspell import Hunspell

##################################################################
# Constants
# maximum number of candidates to be generated
MAX_CANDIDATES = -1
# whether to use checking word validness with hunspell - caution, currently
# it's extremly slow and is subject to be re-written in C or C++.
USE_DICT       = True

##################################################################
# Processing Arguments
# note: some options are already set up by alt_argparser
argparser.description="""Utility for restoring original form of words
with deliberately multiply repeated letters."""
argparser.add_argument("-r", "--skip-re", help = """regular
 expression describing string which should be not processed by the script""", \
                                default = r"(?!)")
argparser.add_argument("-d", "--dictionary", help = """dictionary to be used for
checking words""", default = "de_CH")
argparser.add_file_argument("-t", "--stat-file", help = """additional file with
statistics about usage of certain prolongated form""")
argparser.add_argument("-1", "--single-candidate", help = """perform the
replacement with one of the generated candidates in place instead of outputting
a list of all possible candidates with their probabilities (the default)""", \
                           action = "store_true")
args = argparser.parse_args()

# if statistics file was specified, read it
if args.stat_file:
    import pickle
    stat_file  = args.stat_file
    prob_table = pickle.load(stat_file)
    stat_file.close()
# if no statistics file was specified, all look-ups in prob_table and will
# yield False
else:
    prob_table = {}

##################################################################
# Variables
flsh       = args.flush
skip_line  = args.skip_line
skip_re    = re.compile(args.skip_re)
single     = args.single_candidate
candidates = set([])
foutput    = AltFileOutput(encoding = args.encoding, \
                               flush = args.flush)
finput     = AltFileInput(*args.files, skip_line = args.skip_line, \
                              print_func = foutput.fprint)

##################################################################
# Functions
def has_repetition(iword):
    """Return bool indicating if any squeezing should be performed at all."""
    # TODO: check if sequence of repeated characters is located on the
    # boundary of a compound
    return repeated_chars.THREE_LETTERS_RE.search(iword) and \
        not repeated_chars.LEGAL_REPETITION_RE.match(iword)

def generate_candidates_helper(iword, pos = 0):
    """Look for all occurrences of repeated letters and squeeze them."""
    ret   = []
    m_obj = repeated_chars.REPEATED_LETTERS_RE.search(iword[pos:])
    if m_obj:
        start = pos + m_obj.start()
        end   = pos + m_obj.end()
        # iterate on original line with increased pos
        ret += generate_candidates_helper(iword, end)
        # change line and iterate on changed version
        iword = iword[:end - 1] + iword[end:]
        ret += generate_candidates_helper(iword, start)
    else:
        ret.append(iword)
    return ret

def generate_candidates(iword):
    """Generate normalization candidates by squeezing repeating letters."""
    # squeeze occurrences of same letters which repeat more than 3
    # times in sequence to just 3 repeating occurrences
    iword = repeated_chars.GT_THREE_LETTERS_RE.sub(r"\1\2", iword)
    # generate all possible candidates
    return sorted(generate_candidates_helper(iword))[:MAX_CANDIDATES]

# Dictionary is currently switched on, but we are not sure, whether huspell is
# available. So by default, lookup_dict will always return False, but if
# hunspell is available lookup_dict will return whatever Hunspell says.
checkdict  = None
def lookup_dict(iword):
    """Return True if iword is in dictionary, false otherwise."""
    return False                # no lexicon available so far

# is switched on
if USE_DICT:
    try:
        checkdict  = Hunspell(dic = args.dictionary)
        def lookup_dict(iwords):
            """Check if any of iwords are known to dictionary and return bool."""
            return checkdict.spell_list(iwords)
    except OSError:
        print >> sys.stderr, """WARNING: Hunspell is not available on this computer.
Results may differ."""

def equiprobable(icandidates):
    """Assign equal probabilities to all elements of icandidates."""
    # divide 1 by number of candidates
    prob = 1.0 / len(icandidates)
    # assign this probability to all the candidates
    return [(candidate, prob) for candidate in icandidates]

def prune(orig_word, candidates):
    """Filter-out unlikely candidates based on heuristics."""
    # method squeeze() is defined in ld.repeated_chars
    stat_key = repeated_chars.squeeze(orig_word.lower())
    # lookup generated candidates in dictionary
    dictforms = lookup_dict(candidates)
    # if any of the candidates were found in dictionary, return only
    # found candidates
    if dictforms:
        # assign equal probability to all forms found in dictionary
        # (subject to change)
        return equiprobable(dictforms)
    # otherwise, check if we have gathered some statistics for given
    # squeezed form and rely solely on statistics if yes
    # as a default solution, make original word the 1-st candidate in list,
    # then assign equal probabilities to all the list elements and return list.
    elif stat_key in prob_table:
        # if we've seen the value on corpus, return mappings from
        # corpus along with their probabilities. (If no stat_file was
        # specified as argument prob_table will be an empty dict)
        return prob_table[stat_key]
    else:
        # make original word the first candidate in the list of candidates
        if orig_word in candidates:
            orig_word_idx = candidates.index(orig_word)
            # if we found orig word in our candidate list, swap it with the
            # very 1-st list element
            candidates[0], candidates[orig_word_idx] = \
                candidates[orig_word_idx], candidates[0]
        else:
            candidates[0] = orig_word
        return equiprobable(candidates)

##################################################################
# Main
for word in finput:
    # check if we have to skip input word, either because it equals
    # skip_line or has no repetitions in it
    if skip_re.match(word) or not has_repetition(word):
        foutput.fprint(word)
        continue
    # otherwise generate replacement candidates and depending on
    # options return only one, most probable of them
    candidates = prune(word, generate_candidates(word))
    candidates = [(adjust_case(candidate, word), prob) for candidate, prob in \
                      candidates]
    # check if any candidates were generated
    if not candidates:
        foutput.fprint(word)
    # if requested or if only one candidate was generated, output
    # only the first candidate for replacement
    elif single or len(candidates) == 1:
        foutput.fprint(candidates[0][0])
    else:
        stat = u""
        # convert candidates and their probs to strings
        for candidate, prob in candidates:
            stat += u"\t" + candidate + u" " + unicode(prob)
        del candidates
        foutput.fprint(word + stat)

if checkdict:
    checkdict.close()
