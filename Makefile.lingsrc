# -*- mode: makefile; -*-

#############
# Variables #
#############
LINGBIN_DIR := ${SOCMEDIA_LINGBIN}
LINGTMP_DIR := ${SOCMEDIA_LINGTMP}
# Corpus
SRC_CORPUS  := ${SOCMEDIA_LINGSRC}/corpus/twitter_wulff.txt
TRG_CORPUS_DIR := ${SOCMEDIA_LINGTMP}/corpus
DIR_LIST += ${TRG_CORPUS_DIR}
PREPROCESSED_CORPUS := ${TRG_CORPUS_DIR}/preprocessed_corpus.txt
MACROS_FILE:=${SOCMEDIA_LINGSRC}/macros.m4

# Add ${LINGBIN_DIR} to the list of automatically created directories
DIR_LIST += ${LINGBIN_DIR} ${LINGTMP_DIR}

# auxiliary syntax variables
SPACE :=
SPACE +=
COMMA := ,
AUX_SFX := .aux .log .out

###################
# Special Targets #
###################
.PHONY: corpus character_squeezer_stat \
	ngram_stat unigram_stat bigram_stat \
	topics topics_bernoulli topics_multinomial \
	sentiment \
	sentiment_corpus \
	sentiment_corpus_doc \
	sentiment_mmax_conll_corpus \
	sentiment_db_corpus\
	sentiment_corpus_train \
	sentiment_corpus_devtest \
	sentiment_corpus_test \
	sentiment_train \
	train_sentiment \
	sentiment_traintest \
	sentiment_devtest \
	sentiment_test test_sentiment \
	semantic_dict \
	clean_corpus \
	clean_character_squeezer_stat \
	clean_ngram_stat clean_topics \
	clean_sentiment \
	clean_sentiment_train \
	clean_sentiment_traintest \
	clean_sentiment_corpus \
	clean_sentiment_corpus_doc \
	clean_sentiment_conll_corpus \
	clean_sentiment_mmax_conll_corpus \
	clean_sentiment_db_corpus \
	clean_sentiment_train_corpus \
	clean_sentiment_devtest \
	clean_sentiment_devtest_corpus \
	clean_sentiment_test_corpus \
	clean_sentiment_test \
	clean_semantic_dict

.SECONDEXPANSION:

####################
# Specific Targets #
####################
# all_lingsrc
all_lingsrc: corpus character_squeezer_stat \
	ngram_stat topics sentiment

################
# clean_lingsrc
clean_lingsrc: clean_corpus clean_character_squeezer_stat \
	clean_ngram_stat clean_topics clean_sentiment

###############
# help_lingsrc
help_lingsrc:
	-@echo -ne "### Linguistic Resources ###\n\
	all_lingsrc   - compile linguistic components\n\
	corpus  - make preprocessed Twitter corpus\n\
	character_squeezer_stat   - gather statistics necessary for squeezing\n\
			            duplicated characters\n\
	ngram_stat - gather unigram and bigram statistics from corpus\n\
	unigram_stat - gather unigram statistics from corpus\n\
	bigram_stat  - gather bigram statistics from corpus\n\
	sentiment    - compile all data necessary for sentiment module\n\
	sentiment_corpus - prepare sentiment corpus for training and testing ML\n\
	sentiment_corpus_doc - prepare documentation for sentiment corpus\n\
	semantic_dict - prepare dictionaries with semantic class words\n\
	topics       - gather statistics necessary for detection of topics\n\
	\n\
	clean_lingsrc - remove linguistic components\n\
	clean_corpus  - remove preprocessed corpus\n\
	clean_character_squeezer_stat - remove generated stat files for character_squeezer\n\
	clean_ngram_stat - gather unigram and bigram statistics from corpus\n\
	clean_unigram_stat - remove files with unigram statistics\n\
	clean_bigram_stat  - remove files with bigram statistics\n\
	clean_sentiment    - remove all temporary and compiled data pertaining to sentiment module\n\
	clean_sentiment_corpus - remove converted sentiment corpus\n\
	clean_sentiment_corpus_doc - remove documentation for sentiment corpus\n\
	clean_sentiment_conll_corpus - remove sentiment corpus with joined MMAX CONLL information\n\
	clean_sentiment_train_corpus - remove training data for sentiment\n\
	clean_sentiment_devtest_corpus - remove devtest data for sentiment\n\
	clean_sentiment_test_corpus - remove test data for sentiment\n\
	clean_semantic_dict - remove dictionaries with semantic class words\n\
	clean_topics - remove files created by topics\n\n" >&2

######################
# preprocessed_corpus
corpus: ${PREPROCESSED_CORPUS}

${PREPROCESSED_CORPUS}: ${SRC_CORPUS}
	set -e -o pipefail; \
	character_normalizer $^ | noise_cleaner -n | \
	slang_normalizer | umlaut_restorer | gawk 'NF{gsub(/[[:punct:]]+/, " "); \
	sub(/^[[:blank:]]+/, ""); sub(/[[:blank:]]$$/, ""); \
	gsub(/[[:blank:]][[:blank:]]+/, " "); print tolower($$0)}'  > '$@.tmp' && \
	mv '$@.tmp' '$@'

clean_corpus:
	-rm -f ${PREPROCESSED_CORPUS}

##########################
# character_squeezer_stat
CHAR_SQUEEZER_PICKLE := ${LINGBIN_DIR}/lengthened_stat.pckl

character_squeezer_stat: ${LINGBIN_DIR}/lengthened_stat.pckl | \
		    create_dirs

${CHAR_SQUEEZER_PICKLE}: ${PREPROCESSED_CORPUS}
	set -e ; \
	lengthened_stat $^ > '${@}.tmp' && mv '${@}.tmp' '$@'

clean_character_squeezer_stat: clean_corpus
	-rm -f ${LINGBIN_DIR}/lengthened_stat.pckl

#############
# ngram_stat
ngram_stat: unigram_stat bigram_stat

unigram_stat bigram_stat: %: ${LINGBIN_DIR}/%.pckl

unigram_stat: GRAM_SIZE := 1
bigram_stat:  GRAM_SIZE := 2

${LINGBIN_DIR}/unigram_stat.pckl ${LINGBIN_DIR}/bigram_stat.pckl: ${PREPROCESSED_CORPUS}
	set -e -o pipefail; \
	ngram_stat -n ${GRAM_SIZE} $< > $@.tmp && mv $@.tmp $@

clean_ngram_stat: clean_unigram_stat clean_bigram_stat

clean_unigram_stat clean_bigram_stat: clean_%: clean_corpus
	-rm -f ${LINGBIN_DIR}/$*.pckl

#################################
# topics
# number of topics to be distinguished
N_TOPICS := 40
TOPICS_CORPUS := ${TRG_CORPUS_DIR}/topics_corpus.txt
TOPIC_MODEL_PICKLE = ${LINGBIN_DIR}/topics.%.pckl

topics: topics_bernoulli topics_multinomial

topics_bernoulli topics_multinomial: topics_% : ${TOPIC_MODEL_PICKLE}

${TOPIC_MODEL_PICKLE}: ${TOPICS_CORPUS}
	set -e; \
	topics_train_parameters '--model=$*' --number-of-topics=${N_TOPICS} \
	'$<' > '$@.tmp' && mv '$@.tmp' '$@'

${TOPICS_CORPUS}: ${SRC_CORPUS} ${CHAR_SQUEEZER_PICKLE}
	set -e; \
	topics_train_corpus '$<' > '$@.tmp' && \
	mv '${@}.tmp' '$@'

clean_topics:
	-rm -f '${TOPICS_CORPUS}' ${LINGBIN_DIR}/topics*

#######################
# semantic dictionaries
# (macro expansion of SentiWS)
SEMDICT_SRCDIR := ${SOCMEDIA_LINGSRC}/semdict
SEMDICT_SFX   := .smdict
SEMDICT_FILES := $(addprefix ${SOCMEDIA_SEMDICT_DIR}/,$(notdir $(wildcard \
	$(SEMDICT_SRCDIR)/*$(SEMDICT_SFX))))

semdict: create_dirs ${SEMDICT_FILES}

# expand macros, strip comments, and downcase words
${SEMDICT_FILES}: ${SOCMEDIA_SEMDICT_DIR}/%: ${SEMDICT_SRCDIR}/% ${MACROS_FILE}
	set -e -o pipefail; \
	m4 -P -I. ${MACROS_FILE} $< | \
	gawk -F "\t" -v OFS="\t" '/^##!/{print; next} \
	NF {sub(/(^|[[:space:]]+)#.*$$/, "")} $$0 {$$1 = tolower($$1); print}' > \
	"${@}.tmp" && mv "${@}.tmp" "${@}"

clean_semdict:
	-rm -f ${SEMDICT_FILES}

############
# sentiment
sentiment: sentiment_corpus sentiment_train

clean_sentiment: clean_sentiment_corpus \
	clean_sentiment_train

sentiment_corpus: character_squeezer_stat ngram_stat sentiment_corpus_doc

clean_sentiment_corpus: clean_sentiment_corpus_doc \
			clean_sentiment_conll_corpus \
			clean_sentiment_train_corpus \
			clean_sentiment_devtest_corpus \
			clean_sentiment_test_corpus

# `train_sentiment' is just an alias for `sentiment_train'
train_sentiment: sentiment_train

# `clean_train_sentiment' is just an alias for `clean_sentiment_train'
clean_train_sentiment: clean_sentiment_train

#################################
# sentiment corpus
SENTIMENT_CORPUS_ROOT := ${SOCMEDIA_LINGSRC}/corpus/sentiment

# MMAX-related stuff
SENTIMENT_CORPUS_BASE_DIR := ${SENTIMENT_CORPUS_ROOT}/basedata

SENTIMENT_CORPUS_DOC_DIR := ${SENTIMENT_CORPUS_ROOT}/doc
SENTIMENT_CORPUS_DOC := ${SENTIMENT_CORPUS_DOC_DIR}/annotation_guidelines.pdf

SENTIMENT_CORPUS_SRC_DIR := ${SENTIMENT_CORPUS_ROOT}/source
SENTIMENT_CORPUS_SRC := ${SENTIMENT_CORPUS_SRC_DIR}/twitter.sentiment.xml

SENTIMENT_CORPUS_ANNOTATOR ?= 2
SENTIMENT_CORPUS_ANNO_DIR := ${SENTIMENT_CORPUS_ROOT}/annotator-${SENTIMENT_CORPUS_ANNOTATOR}
SENTIMENT_CORPUS_MRKBL_DIR := ${SENTIMENT_CORPUS_ANNO_DIR}/markables

SENTIMENT_CORPUS_ORIG_SFX := .xml
SENTIMENT_CORPUS_ORIG_FILES := $(wildcard $(SENTIMENT_CORPUS_SRC_DIR)/*$(SENTIMENT_CORPUS_ORIG_SFX))
# remove raw twitter corpus from the list of original files
SENTIMENT_CORPUS_ORIG_FILES := $(filter-out $(SENTIMENT_CORPUS_SRC),$(SENTIMENT_CORPUS_ORIG_FILES))

SENTIMENT_CORPUS_BASE_SFX := .words${SENTIMENT_CORPUS_ORIG_SFX}

# target directory for storing learned models and training and testing
# datasets
SENTIMENT_TRGDIR := ${LINGTMP_DIR}/sentiment

# corpus with CONLL trees
SENTIMENT_CONLL_DIR := ${SENTIMENT_TRGDIR}/conll
DIR_LIST += ${SENTIMENT_CONLL_DIR}
# source corpus with bare DG features
SENTIMENT_CONLL_CORPUS := ${SENTIMENT_CONLL_DIR}/corpus.raw.conll

sentiment_corpus_doc: ${SENTIMENT_CORPUS_DOC}

${SENTIMENT_CORPUS_DOC}: %.pdf: %.tex
	set -e; \
	cd ${@D} && pdflatex $< && pdflatex $<

clean_sentiment_corpus_doc:
	-rm -f ${SENTIMENT_CORPUS_DOC} $(foreach sfx,$(AUX_SFX),$(wildcard \
	$(SENTIMENT_CORPUS_DOC_DIR)/*$(sfx)))

# extract DG trees from raw unannotated sentiment corpus
${SENTIMENT_CONLL_CORPUS}: ${SENTIMENT_CORPUS_SRC}
	set -e -o pipefail; \
	xml2tsv $< | TextTagger --no-lang-filter | TextParser -t > $@.tmp && \
	mv $@.tmp $@

# for each original MMAX file, generate a corresponding CONLL file by
# merging CONLL analysis with MMAX annotation
SENTIMENT_MMAX_CONLL_SFX := .mmaxconll
SENTIMENT_MMAX_CONLL_FILES := $(addprefix $(SENTIMENT_CONLL_DIR)/,\
$(addsuffix $(SENTIMENT_MMAX_CONLL_SFX),$(notdir $(basename $(SENTIMENT_CORPUS_ORIG_FILES)))))

# Split annotated source data into train, devtest, and test parts
SENTIMENT_CORPUS_TRAIN_PART := $(shell get_range -70 $(SENTIMENT_MMAX_CONLL_FILES))
SENTIMENT_CORPUS_DEVTEST_PART := $(shell get_range 70-85 $(SENTIMENT_MMAX_CONLL_FILES))
SENTIMENT_CORPUS_TEST_PART := $(shell get_range 85- $(SENTIMENT_MMAX_CONLL_FILES))

# use static pattern rule, to generate files with joined MMAX CONLL
# information, these files will later be used for various classifiers
sentiment_mmax_conll_corpus: ${SENTIMENT_MMAX_CONLL_FILES}

${SENTIMENT_MMAX_CONLL_FILES}: ${SENTIMENT_CONLL_DIR}/%${SENTIMENT_MMAX_CONLL_SFX}: \
	${SENTIMENT_CONLL_CORPUS} ${SENTIMENT_CORPUS_SRC_DIR}/%${SENTIMENT_CORPUS_ORIG_SFX} \
	${SENTIMENT_CORPUS_BASE_DIR}/%${SENTIMENT_CORPUS_BASE_SFX} \
	$$(wildcard $(SENTIMENT_CORPUS_MRKBL_DIR)/%_*)
	if test -z '$(word 4,$^)'; then \
	    echo 'No markables files found for file $@' >&2; \
	    exit 1; \
	else \
	    set -e -o pipefail; merge_conll_mmax.py $^ > $@.tmp && mv $@.tmp $@; \
	fi

clean_sentiment_conll_corpus: clean_sentiment_mmax_conll_corpus
	-rm -f ${SENTIMENT_CONLL_CORPUS}

clean_sentiment_mmax_conll_corpus:
	-rm -f  ${SENTIMENT_MMAX_CONLL_FILES}

#################################
# sentiment training and testing (different ML frameworks are used)

# select a ML model for training and testing
SENTIMENT_TRAIN_MODE ?= CRF

ifeq "${SENTIMENT_TRAIN_MODE}" "MLN"
# directory for storing DB files
SENTIMENT_DATA_TRGDIR := ${SENTIMENT_TRGDIR}/mln

# suffix of files in DB format
SENTIMENT_DATA_DBSFX := .db
# suffix of files with non-evidence predicates
SENTIMENT_DATA_NESFX := .ne
# suffix of files with evidence predicates
SENTIMENT_DATA_EVSFX := .ev
# non-evidence predicates will be needed for training and testing
SENTIMENT_NONEVIDENCE_PRED ?= isSentiment,hasSentimentPolarity,isTarget,isSource
# be cautious and do not put any special regexp characters in the name
# of the predicates
SENTIMENT_NONEVIDENCE_PRED_RE := $(subst $(COMMA),|,$(SENTIMENT_NONEVIDENCE_PRED))

# directory for storing merged CONLL MMAX data converted to DB format
SENTIMENT_DATA_DBDIR := ${SENTIMENT_DATA_TRGDIR}/db

# directories for storing training, development, and test datasets
SENTIMENT_DATA_TRAINDIR := ${SENTIMENT_DATA_TRGDIR}/train
SENTIMENT_DATA_DEVTEST_DIR := ${SENTIMENT_DATA_TRGDIR}/devtest
SENTIMENT_DATA_TEST_DIR := ${SENTIMENT_DATA_TRGDIR}/test

# add directories to the list of automatically created directories
DIR_LIST += ${SENTIMENT_DATA_TRGDIR} ${SENTIMENT_DATA_DBDIR} \
	${SENTIMENT_DATA_TRAINDIR} ${SENTIMENT_DATA_DEVTEST_DIR} \
	${SENTIMENT_DATA_TEST_DIR}

# Each merged MMAX CONLL file should first be converted to the DB
# format and stored in the ${SENTIMENT_DATA_TRGDIR}/db/ directory.
# Next, we should extract all tweet id's present in the generated db
# file and generate exactly one file for each tweet in the
# corresponding dataset directory.  This tweet file will only depend
# on the DB file which it was generated from.  Since these procedures
# are the same for training, devtest, and test parts, we put them into
# a macro, and then call this macro for each of the set files.

# template for generating single db input file for each tweet
define GENERATE_CORPUS_FILES
dbfname := $$(addprefix $(SENTIMENT_DATA_DBDIR)/,\
$$(addsuffix $(SENTIMENT_DATA_DBSFX),$$(notdir $$(basename $1))))

# extract all tweet id's from __fname and convert them to file names
# for training, devtest, or test set
tmp_flist := $$(addprefix $2/,$$(addsuffix \
$(SENTIMENT_DATA_DBSFX),$$(shell get_tweet_ids $1)))

# make the new files in `$${tmp_flist}` depend on `dbfname` and
# specify a recipe for their creation
$${tmp_flist}: $${dbfname}
	set -e -o pipefail; \
	get_chunk_by_id --id=$$(basename $$(notdir $$@)) $$< > $$@.tmp && \
	mv $$@.tmp $$@

# in TEST_MODE, we will split all database files into those with
# evidence and those with non-evidence predicates
ifeq "$${TEST_MODE}" "1"
# for each `.db' file, there will be a corresponding `.ne' and
# `.ev'file with non-evidence and evidence predicates respectively
ev_flist := $$(addsuffix $(SENTIMENT_DATA_EVSFX),$$(basename $$(tmp_flist)))
ne_flist := $$(addsuffix $(SENTIMENT_DATA_NESFX),$$(basename $$(tmp_flist)))

# Both evidence and non-evidence files will be generated from one
# `.db' file by using a static-pattern rule.  In test mode, each
# evidence clause will be made a hard clause.
$${ev_flist}: %${SENTIMENT_DATA_EVSFX}: %${SENTIMENT_DATA_DBSFX}
	egrep -vw '${SENTIMENT_NONEVIDENCE_PRED_RE}' < $$< | \
	sed -e 's/.$$$$/&./' > $$@.tmp ; mv $$@.tmp $$@

$${ne_flist}: %${SENTIMENT_DATA_NESFX}: %${SENTIMENT_DATA_DBSFX}
	egrep -w '${SENTIMENT_NONEVIDENCE_PRED_RE}' < $$< > $$@.tmp ; \
	mv $$@.tmp $$@

# `tmp_flist` will actually store only evidence and non-evidence
# files, `.db' files will be considered temporary proxies.
tmp_flist := $${ne_flist} $${ev_flist}
endif
# add newly generated file names to the specified variable
$3 += $${tmp_flist}
endef

# actually generate rules for the training corpus
TEST_MODE := 0
SENTIMENT_TRAIN_CORPUS :=
$(foreach fname,$(SENTIMENT_CORPUS_TRAIN_PART),\
$(eval $(call GENERATE_CORPUS_FILES,$(fname),$(SENTIMENT_DATA_TRAINDIR),\
SENTIMENT_TRAIN_CORPUS)))
SENTIMENT_TRAIN_FLIST := ${SENTIMENT_DATA_TRAINDIR}/train_file_list

# DEVTEST and TEST corpora will have two files for each tweet, one
# with evidence predicates and one with only non-evidence predicates
TEST_MODE := 1
# generate rules for devtest corpus
SENTIMENT_DEVTEST_CORPUS :=
$(foreach fname,$(SENTIMENT_CORPUS_DEVTEST_PART),\
$(eval $(call GENERATE_CORPUS_FILES,$(fname),$(SENTIMENT_DATA_DEVTEST_DIR),\
SENTIMENT_DEVTEST_CORPUS)))

# generate rules for test corpus
SENTIMENT_TEST_CORPUS :=
$(foreach fname,$(SENTIMENT_CORPUS_TEST_PART),\
$(eval $(call GENERATE_CORPUS_FILES,$(fname),$(SENTIMENT_DATA_TEST_DIR),\
SENTIMENT_TEST_CORPUS)))

# actual corpus rules and dependencies
sentiment_corpus: sentiment_db_corpus ${SENTIMENT_TRAIN_FLIST} \
	${SENTIMENT_TRAIN_CORPUS} ${SENTIMENT_DEVTEST_CORPUS} \
	${SENTIMENT_TEST_CORPUS}

# do the clean-up (need find + xargs due to a large number of files)
clean_sentiment_corpus: clean_sentiment_db_corpus \
	clean_sentiment_train_corpus \
	clean_sentiment_devtest_corpus \
	clean_sentiment_test_corpus

####################
# DB files

# convert files with MMAX CONLL information to format appropriate for
# DB
SENTIMENT_DB_FILES := $(addprefix $(SENTIMENT_DATA_DBDIR)/,\
$(addsuffix $(SENTIMENT_DATA_DBSFX),$(notdir $(basename \
$(SENTIMENT_MMAX_CONLL_FILES)))))

sentiment_db_corpus: ${SENTIMENT_DB_FILES}

# SENTIMENT_DB_FILES are generated from MMAX CONLL files, by simply
# converting them into DB format
${SENTIMENT_DB_FILES}: ${SENTIMENT_DATA_DBDIR}/%${SENTIMENT_DATA_DBSFX}: \
	${SENTIMENT_CONLL_DIR}/%${SENTIMENT_MMAX_CONLL_SFX}
	set -e -o pipefail; conll2db $^ > $@.tmp && mv $@.tmp $@

# remove database files
clean_sentiment_db_corpus:
	-rm -f ${SENTIMENT_DB_FILES}

####################
# clean rules for train, devtest, and test sets

# rules for creation of training, devtest, and test corpora have
# already been created previously
clean_sentiment_train_corpus: clean_sentiment_train
	-rm -f '${SENTIMENT_TRAIN_FLIST}' && find '${SENTIMENT_DATA_TRAINDIR}' \
	-type f -print0 | xargs -0 rm -f

clean_sentiment_devtest_corpus: clean_sentiment_devtest
	-find '${SENTIMENT_DATA_DEVTEST_DIR}' -type f -print0 | xargs -0 rm -f

clean_sentiment_test_corpus: clean_sentiment_test
	-find '${SENTIMENT_DATA_TEST_DIR}' -type f -print0 | xargs -0 rm -f

#################################
# file with a list of all training data (need to use `find` here,
# because `SENTIMENT_TRAIN_CORPUS` list is too large to fit into the
# maximum length of arguments)
${SENTIMENT_TRAIN_FLIST}: ${SENTIMENT_TRAIN_CORPUS}
	set -e -o pipefail; \
	find ${<D} -type f -name '*$(suffix $<)' > $@.tmp && mv $@.tmp $@

#################################
# sentiment training
SENTIMENT_MLN_MAIN := ${SOCMEDIA_LINGSRC}/sentiment/mln/main.mln
SENTIMENT_MLN_TYPES := '\#include lingsrc/sentiment/mln/types.mln'
SENTIMENT_RES_MODEL  := ${SENTIMENT_TRGDIR}/sentiment.mln
# closed world is needed for -lazy training
CLOSEDWORLD_PRED ?= -cw \
isEmoexpression,hasEmoexpressionPolarity,hasContextualPolarity,isIntensifier,isDiminisher,isNegation,Connector,isCase,isDegree,isGender,isMood,isNumber,isPerson,isTense
DIR_LIST += ${SENTIMENT_RES_DIR}

sentiment_train: ${SENTIMENT_RES_MODEL}

${SENTIMENT_RES_MODEL}: ${SENTIMENT_MLN_MAIN} ${SENTIMENT_TRAIN_FLIST} ${SENTIMENT_TRAIN_CORPUS}
	set -e -o pipefail; \
	learnwts -i $< -o $@.tmp -l '${SENTIMENT_TRAIN_FLIST}' \
	-multipleDatabases -ms -d -ne '${SENTIMENT_NONEVIDENCE_PRED}' ${CLOSEDWORLD_PRED} \
	-queryEvidence -dMaxMin 840.0 -dLearningRate 1 -memLimit 10240 -lazy && \
	echo ${SENTIMENT_MLN_TYPES} >> $@.tmp && \
	mv $@.tmp $@

clean_sentiment_train:
	-rm -f ${SENTIMENT_RES_MLN}

#################################
# sentiment testing (evaluate model's performance on the devtest and test set)
SENTIMENT_CORPUS_RESSFX := .res
SENTIMENT_CORPUS_CMPSFX := .cmp

SENTIMENT_DEVTEST_CMP_FILES := $(addsuffix $(SENTIMENT_CORPUS_CMPSFX),\
	$(basename $(filter %$(SENTIMENT_DATA_EVSFX),$(SENTIMENT_DEVTEST_CORPUS))))

SENTIMENT_TEST_CMP_FILES := $(addsuffix $(SENTIMENT_CORPUS_CMPSFX),\
	$(basename $(filter %$(SENTIMENT_DATA_EVSFX),$(SENTIMENT_TEST_CORPUS))))

# Both, sentiment_devtest and sentiment_test targets will depend on
# corresponding `.res' files, which are results of inference run on
# evidence files.  Additionally, each target will depend on a dummy
# phony target, to make sure that evalutaion tests are re-run each
# time.
# make a summary from information in all cmp files
sentiment_devtest: ${SENTIMENT_DEVTEST_CMP_FILES}
	 @echo 'Testing $@' && sort -t "	" -k1,1d $^ | summarize_cmp

# remove all cmp and res files found in `${SENTIMENT_DEVTEST_DIR}'
clean_sentiment_devtest:
	-find ${SENTIMENT_CORPUS_DEVTEST_DIR} \( -name '*${SENTIMENT_CORPUS_RESSFX}' -o \
	-name '*${SENTIMENT_CORPUS_CMPSFX}' \) -print0 | xargs -0 rm -f

# make a summary from information in all cmp files
sentiment_test: ${SENTIMENT_TEST_CMP_FILES}
	@echo 'Testing $@' && sort -t "	" -k1,1d $^ | summarize_cmp

# remove all cmp and res files found in `${SENTIMENT_TEST_DIR}'
clean_sentiment_test:
	-find ${SENTIMENT_CORPUS_TEST_DIR} \( -name '*${SENTIMENT_CORPUS_RESSFX}' -o \
	-name '*${SENTIMENT_CORPUS_CMPSFX}' \)  -print0 | xargs -0 rm -f

# each cmp file will depend on a non-evidence and result file, and
# will be produced by comparing them using an implicit rule
${SENTIMENT_DEVTEST_CMP_FILES} ${SENTIMENT_TEST_CMP_FILES}: %${SENTIMENT_CORPUS_CMPSFX}: \
	%${SENTIMENT_DATA_DBSFX} %${SENTIMENT_CORPUS_RESSFX}
	set -e -o pipefail; \
	cmp_mln_res $^ > $@.tmp && mv $@.tmp $@

# all res files will be generated with a static pattern rule
SENTIMENT_INFERENCE ?=
%${SENTIMENT_CORPUS_RESSFX}: %${SENTIMENT_DATA_EVSFX} ${SENTIMENT_RES_MLN}
	set -e -o pipefail; \
	infer ${SENTIMENT_INFERENCE} -i ${SENTIMENT_RES_MLN} -e $< -r	\
	$@.tmp -q '${SENTIMENT_NONEVIDENCE_PRED}' && mv $@.tmp $@

# train and test CRF
else ifeq "${SENTIMENT_TRAIN_MODE}" "CRF"

# directory for storing DB files
SENTIMENT_DATA_TRGDIR := ${SENTIMENT_TRGDIR}/crf

# suffix for CRF data
SENTIMENT_DATA_SFX := .crf

# directories for storing training, development, and test datasets
SENTIMENT_DATA_TRAIN_DIR := ${SENTIMENT_DATA_TRGDIR}/train
SENTIMENT_DATA_DEVTEST_DIR := ${SENTIMENT_DATA_TRGDIR}/devtest
SENTIMENT_DATA_TEST_DIR := ${SENTIMENT_DATA_TRGDIR}/test

# add directories to the list of automatically created directories
DIR_LIST += ${SENTIMENT_DATA_TRGDIR} ${SENTIMENT_DATA_TRAIN_DIR} \
	${SENTIMENT_DATA_DEVTEST_DIR} ${SENTIMENT_DATA_TEST_DIR}

# training, devtest, and test corpora will be obtained from the
# respective parts of original files.  All the files will later be
# generated using one macro.
SENTIMENT_TRAIN_CORPUS := $(addprefix $(SENTIMENT_DATA_TRAIN_DIR)/,\
	$(addsuffix $(SENTIMENT_DATA_SFX),$(basename $(notdir $(SENTIMENT_CORPUS_TRAIN_PART)))))

SENTIMENT_DEVTEST_CORPUS := $(addprefix $(SENTIMENT_DATA_DEVTEST_DIR)/,\
	$(addsuffix $(SENTIMENT_DATA_SFX),$(basename $(notdir $(SENTIMENT_CORPUS_DEVTEST_PART)))))

SENTIMENT_TEST_CORPUS := $(addprefix $(SENTIMENT_DATA_TEST_DIR)/,\
	$(addsuffix $(SENTIMENT_DATA_SFX),$(basename $(notdir $(SENTIMENT_CORPUS_TEST_PART)))))

###################
# Building Macros
define SENTIMENT_GET_CORPUS
	set -e -o pipefail; \
	$<  ${FEAT_CONVERT_FLAGS} $(filter %$(SENTIMENT_MMAX_CONLL_SFX),$^) > $@.tmp && mv $@.tmp $@
endef

define SENTIMENT_TEST
	set -e -o pipefail; \
	SentimentExtractor --preprocessed ${CRF_TEST_FLAGS} $(wordlist 2,$(words $^),$^) | \
	crf_evaluate ${SENTIMENT_TEST_FLAGS}
endef

###################
# sentiment_corpus
sentiment_corpus: sentiment_train_corpus \
	sentiment_devtest_corpus \
	sentiment_test_corpus

clean_sentiment_corpus: clean_sentiment_train_corpus \
	clean_sentiment_devtest_corpus \
	clean_sentiment_test_corpus

###################
# sentiment_train
SENTIMENT_FEAT_PROCESSOR := ${SCRIPT_DIR}/conll2crf
SENTIMENT_RES_MODEL := ${SENTIMENT_TRGDIR}/sentiment.crf
# Settings for annotator 1
ifeq '${SENTIMENT_CORPUS_ANNOTATOR}' '1'
CRF_TRAIN_FLAGS += -p feature.possible_transitions=1 -p feature.possible_states=1 \
	-p c1=0.06 -p c2=0.09 -p feature.minfreq=2
CRF_TEST_FLAGS ?=
FEAT_CONVERT_FLAGS ?=
else ifeq '${SENTIMENT_CORPUS_ANNOTATOR}' '2'
# Settings for annotator 2
CRF_TRAIN_FLAGS += -p feature.possible_transitions=1 -p feature.possible_states=1 \
	-p c1=0.05 -p c2=0.0009 -p feature.minfreq=2
CRF_TEST_FLAGS ?=
FEAT_CONVERT_FLAGS ?=
else
$(error "Unknown value for sentiment corpus annotator: $(SENTIMENT_CORPUS_ANNOTATOR)")
endif

sentiment_train: ${SENTIMENT_RES_MODEL}

${SENTIMENT_RES_MODEL}: ${SENTIMENT_TRAIN_CORPUS} | ${CRF_BIN_FILE}
	set -e -o pipefail; \
	${CRF_BIN_FILE} learn ${CRF_TRAIN_FLAGS} -m $@.tmp $^ && mv $@.tmp $@

clean_sentiment_train:
	-rm -rf $(wildcard $(SENTIMENT_DATA_TRAIN_DIR)/*$(SENTIMENT_DATA_SFX))

sentiment_train_corpus: ${SENTIMENT_TRAIN_CORPUS}

${SENTIMENT_TRAIN_CORPUS}: ${SENTIMENT_DATA_TRAIN_DIR}/%${SENTIMENT_DATA_SFX}: \
	${SENTIMENT_FEAT_PROCESSOR} \
	${SENTIMENT_CONLL_DIR}/%${SENTIMENT_MMAX_CONLL_SFX} ${SEMDICT_FILES}
	${SENTIMENT_GET_CORPUS}

clean_sentiment_train_corpus: clean_sentiment_train
	-rm -f ${SENTIMENT_TRAIN_CORPUS}

###################
# sentiment_traintest
sentiment_traintest: ${SENTIMENT_RES_MODEL} ${SENTIMENT_TRAIN_CORPUS} | ${CRF_BIN_FILE}
	${SENTIMENT_TEST}

clean_sentiment_traintest:

###################
# sentiment_devtest
sentiment_devtest: ${SENTIMENT_RES_MODEL} ${SENTIMENT_DEVTEST_CORPUS} | ${CRF_BIN_FILE}
	${SENTIMENT_TEST}

clean_sentiment_devtest:

sentiment_devtest_corpus: ${SENTIMENT_DEVTEST_CORPUS}

${SENTIMENT_DEVTEST_CORPUS}: ${SENTIMENT_DATA_DEVTEST_DIR}/%${SENTIMENT_DATA_SFX}: \
	${SENTIMENT_FEAT_PROCESSOR} ${SENTIMENT_CONLL_DIR}/%${SENTIMENT_MMAX_CONLL_SFX} \
	${SEMDICT_FILES}
	${SENTIMENT_GET_CORPUS}

clean_sentiment_devtest_corpus: clean_sentiment_devtest
	-rm -f $(wildcard $(SENTIMENT_DATA_DEVTEST_DIR)/*$(SENTIMENT_DATA_SFX))

###################
# sentiment_test
sentiment_test: ${SENTIMENT_RES_MODEL} ${SENTIMENT_TEST_CORPUS} | ${CRF_BIN_FILE}
	${SENTIMENT_TEST}

clean_sentiment_test:

sentiment_test_corpus: ${SENTIMENT_TEST_CORPUS}

${SENTIMENT_TEST_CORPUS}: ${SENTIMENT_DATA_TEST_DIR}/%${SENTIMENT_DATA_SFX}: \
	${SENTIMENT_FEAT_PROCESSOR} ${SENTIMENT_CONLL_DIR}/%${SENTIMENT_MMAX_CONLL_SFX} \
	${SEMDICT_FILES}
	${SENTIMENT_GET_CORPUS}

clean_sentiment_test_corpus: clean_sentiment_test
	-rm -f $(wildcard $(SENTIMENT_DATA_TEST_DIR)/*$(SENTIMENT_DATA_SFX))

else ifeq "${SENTIMENT_TRAIN_MODE}" "WEKA"
# suffix for training and testing data
SENTIMENT_DATA_SFX := .arff
SENTIMENT_AUX_SFX := .aux

# directory for storing DB files
SENTIMENT_DATA_TRGDIR := ${SENTIMENT_TRGDIR}/weka

# directories for training, development, and test datasets
SENTIMENT_DATA_TRAIN_DIR := ${SENTIMENT_DATA_TRGDIR}/train
SENTIMENT_DATA_DEVTEST_DIR := ${SENTIMENT_DATA_TRGDIR}/devtest
SENTIMENT_DATA_TEST_DIR := ${SENTIMENT_DATA_TRGDIR}/test
SENTIMENT_DATA_AUX_DIR := ${SENTIMENT_DATA_TRGDIR}/aux

# add directories to the list of automatically created directories
DIR_LIST += ${SENTIMENT_DATA_TRAIN_DIR} ${SENTIMENT_DATA_DEVTEST_DIR} \
	${SENTIMENT_DATA_TEST_DIR} ${SENTIMENT_DATA_AUX_DIR}

# Training, devtest, and test corpora will be represented by single files.
SENTIMENT_TRAIN_CORPUS := $(SENTIMENT_DATA_TRAIN_DIR)/train$(SENTIMENT_DATA_SFX)
SENTIMENT_DEVTEST_CORPUS := $(SENTIMENT_DATA_DEVTEST_DIR)/devtest$(SENTIMENT_DATA_SFX)
SENTIMENT_TEST_CORPUS := $(SENTIMENT_DATA_TEST_DIR)/test$(SENTIMENT_DATA_SFX)

# Auxiliary files used for generating corpora (these files will be
# obtained from MMAX CONNLL files by applying)
SENTIMENT_CMNTEST_CORPUS := ${SENTIMENT_DATA_AUX_DIR}/cmntest${SENTIMENT_AUX_SFX}
SENTIMENT_PRETRAIN_CORPUS := ${SENTIMENT_DATA_AUX_DIR}/pretrain$(SENTIMENT_DATA_SFX)
SENTIMENT_PRECMNTEST_CORPUS := ${SENTIMENT_DATA_AUX_DIR}/precmntest$(SENTIMENT_DATA_SFX)

# Separator which separates devtest from test set
SENTIMENT_CMNTEST_SEPARATOR := %%% TEST SET SEPARATOR

# script for extracting feature values
SENTIMENT_FEAT_PROCESSOR := ${SCRIPT_DIR}/conll2weka

# commands for data conversion, training, and classification
SENTIMENT_WEKA_FILTER = java weka.filters.MultiFilter -F weka.filters.unsupervised.attribute.StringToWordVector -F 'weka.filters.unsupervised.attribute.Reorder -R 6-last,5'
SENTIMENT_WEKA_MODEL ?= SVM

ifeq "${SENTIMENT_WEKA_MODEL}" "SVM"
SENTIMENT_SVM_BIN_TRAIN := java weka.classifiers.functions.SMO
SENTIMENT_SVM_BIN_TEST := java weka.classifiers.functions.SMO
# flags for training and testing
SENTIMENT_SVM_TRAIN_FLAGS := -C 1.0 -L 0.001 -P 1.0E-12 -N 0 -V -1 -W 1 \
-K 'weka.classifiers.functions.supportVector.PolyKernel -C 250007 -E 1.0' -no-cv
SENTIMENT_SVM_TEST_FLAGS := -i
# resulting trained model
SENTIMENT_RES_MODEL := ${SENTIMENT_TRGDIR}/sentiment.svm

else
$(error "Invalid value for SENTIMENT_WEKA_MODEL")

endif

##########
# Macros #
##########
define SENTIMENT_TEST
	set -e -o pipefail; \
	${SENTIMENT_SVM_BIN_TEST} ${SENTIMENT_SVM_TEST_FLAGS} -T $(lastword $^) \
	-l $<
endef

#####################
# sentiment corpora
sentiment_corpus: sentiment_train_corpus \
	sentiment_devtest_corpus \
	sentiment_test_corpus

clean_sentiment_corpus: clean_sentiment_train_corpus \
	clean_sentiment_devtest_corpus \
	clean_sentiment_test_corpus

# Sentiment train, devtest, and test corpora will be all generated in
# a single shot, since the same StringToWordVector filter should be
# applied to all input data.
sentiment_train_corpus: ${SENTIMENT_TRAIN_CORPUS}
sentiment_devtest_corpus: ${SENTIMENT_DEVTEST_CORPUS}
sentiment_test_corpus: ${SENTIMENT_TEST_CORPUS}

# apply WEKA filter to features extracted from corpora
${SENTIMENT_TRAIN_CORPUS} ${SENTIMENT_CMNTEST_CORPUS}: ${SENTIMENT_PRETRAIN_CORPUS} \
	${SENTIMENT_PRECMNTEST_CORPUS}
	set -e -o pipefail; \
	${SENTIMENT_WEKA_FILTER} -i ${SENTIMENT_PRETRAIN_CORPUS} -o \
	${SENTIMENT_TRAIN_CORPUS}.tmp -r ${SENTIMENT_PRECMNTEST_CORPUS} \
	-s ${SENTIMENT_CMNTEST_CORPUS}.tmp -b && \
	mv ${SENTIMENT_TRAIN_CORPUS}.tmp ${SENTIMENT_TRAIN_CORPUS} && \
	mv ${SENTIMENT_CMNTEST_CORPUS}.tmp ${SENTIMENT_CMNTEST_CORPUS}

${SENTIMENT_PRETRAIN_CORPUS}: ${SENTIMENT_FEAT_PROCESSOR} $(SENTIMENT_CORPUS_TRAIN_PART)
	set -e -o pipefail; \
	${SENTIMENT_FEAT_PROCESSOR} $(wordlist 2,$(words $^),$^) > $@.tmp && \
	mv $@.tmp $@

${SENTIMENT_PRECMNTEST_CORPUS}: ${SENTIMENT_FEAT_PROCESSOR} ${SENTIMENT_CORPUS_DEVTEST_PART} \
	${SENTIMENT_CORPUS_TEST_PART}
	set -e -o pipefail; \
	{ \
	    ${SENTIMENT_FEAT_PROCESSOR} ${SENTIMENT_CORPUS_DEVTEST_PART}; \
	    echo '${SENTIMENT_CMNTEST_SEPARATOR}'; \
	    ${SENTIMENT_FEAT_PROCESSOR} --no-header ${SENTIMENT_CORPUS_TEST_PART}; \
	} > $@.tmp && mv $@.tmp $@

# Sentiment devtest and sentiment test corpora will be extracted from
# a single file.  Sentiment devtest will get everything in this file
# up to a separator, and the test corpus will get everything in the
# file what comes after the separator string.
${SENTIMENT_DEVTEST_CORPUS}: ${SENTIMENT_CMNTEST_CORPUS}
	set -e -o pipefail; \
	gawk '/^${SENTIMENT_CMNTEST_SEPARATOR}/{exit} 1' < $< > $@.tmp; \
	mv $@.tmp $@

${SENTIMENT_TEST_CORPUS}: ${SENTIMENT_CMNTEST_CORPUS}
	set -e -o pipefail; \
	gawk -v IGNORECASE=1 '1,/^@data/; /^${SENTIMENT_CMNTEST_SEPARATOR}/,1' < $< > $@.tmp; \
	mv $@.tmp $@

# clean corpora
clean_sentiment_train_corpus: clean_sentiment_train
	-rm -f ${SENTIMENT_TRAIN_CORPUS} ${SENTIMENT_PRETRAIN_CORPUS}

clean_sentiment_devtest_corpus: clean_sentiment_devtest
	-rm -f ${SENTIMENT_DEVTEST_CORPUS} ${SENTIMENT_CMNTEST_CORPUS} \
	${SENTIMENT_PRECMNTEST_CORPUS}

clean_sentiment_test_corpus: clean_sentiment_test
	-rm -f ${SENTIMENT_TEST_CORPUS} ${SENTIMENT_CMNTEST_CORPUS} \
	${SENTIMENT_PRECMNTEST_CORPUS}

#####################
# sentiment training
sentiment_train: ${SENTIMENT_RES_MODEL}

${SENTIMENT_RES_MODEL}: ${SENTIMENT_TRAIN_CORPUS} | \
	${WEKA_LIB} ${LSVM_LIBS}
	set -e -o pipefail; \
	${SENTIMENT_SVM_BIN_TRAIN} ${SENTIMENT_SVM_TRAIN_FLAGS} -d $@.tmp -t '$<' && \
	mv $@.tmp $@

clean_sentiment_train:
	-rm -rf ${SENTIMENT_RES_MODEL}

####################
# sentiment testing
sentiment_traintest: ${SENTIMENT_RES_MODEL} ${SENTIMENT_TRAIN_CORPUS}
	${SENTIMENT_TEST}

clean_sentiment_traintest:

sentiment_devtest: ${SENTIMENT_RES_MODEL} ${SENTIMENT_DEVTEST_CORPUS}
	${SENTIMENT_TEST}

clean_sentiment_devtest:

sentiment_test: ${SENTIMENT_RES_MODEL} ${SENTIMENT_TEST_CORPUS}
	${SENTIMENT_TEST}

clean_sentiment_test:

# unknown training mode
else
$(error "Unknown value of SENTIMENT_TRAIN_MODE: '${SENTIMENT_TRAIN_MODE}'")
endif
