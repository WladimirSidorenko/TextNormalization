#############
# Variables #
#############
LINGBIN_DIR := ${SOCMEDIA_LINGBIN}
LINGTMP_DIR := ${SOCMEDIA_LINGTMP}
# Corpus
SRC_CORPUS  := ${SOCMEDIA_LINGSRC}/corpus/twitter_wulff.txt
PREPROCESSED_CORPUS := ${LINGTMP_DIR}/preprocessed_corpus.txt

# Add ${LINGBIN_DIR} to the list of automatically created directories
DIR_LIST += ${LINGBIN_DIR} ${LINGTMP_DIR}

# auxiliary syntax variables
SPACE :=
SPACE +=
COMMA := ,

###################
# Special Targets #
###################
.PHONY: corpus character_squeezer_stat \
	ngram_stat unigram_stat bigram_stat \
	topics topics_bernoulli topics_multinomial \
	sentiment \
	sentiment_corpus \
	sentiment_db_corpus\
	sentiment_corpus_train \
	sentiment_corpus_devtest \
	sentiment_corpus_test \
	sentiment_train train_sentiment \
	sentiment_test test_sentiment \
	sentiment_dict \
	clean_corpus \
	clean_character_squeezer_stat \
	clean_ngram_stat clean_topics \
	clean_sentiment \
	clean_sentiment_train \
	clean_sentiment_corpus \
	clean_sentiment_db_corpus \
	clean_sentiment_corpus_train \
	clean_sentiment_corpus_devtest \
	clean_sentiment_corpus_test \
	clean_sentiment_dict

.SECONDEXPANSION:

####################
# Specific Targets #
####################
# all_lingsrc
all_lingsrc: corpus character_squeezer_stat \
	ngram_stat topics sentiment

################
# clean_lingsrc
clean_lingsrc: clean_corpus clean_character_squeezer_stat \
	clean_ngram_stat clean_topics clean_sentiment

###############
# help_lingsrc
help_lingsrc:
	-@echo -ne "### Linguistic Resources ###\n\
	all_lingsrc   - compile linguistic components\n\
	corpus  - make preprocessed Twitter corpus\n\
	character_squeezer_stat   - gather statistics necessary for squeezing\n\
			            duplicated characters\n\
	ngram_stat - gather unigram and bigram statistics from corpus\n\
	unigram_stat - gather unigram statistics from corpus\n\
	bigram_stat  - gather bigram statistics from corpus\n\
	sentiment    - compile all data necessary for sentiment module\n\
	sentiment_corpus - prepare sentiment corpus for training and testing Alchemy\n\
	sentiment_dict - prepare list of sentiment polarity markers\n\
	topics       - gather statistics necessary for detection of topics\n\
	\n\
	clean_lingsrc - remove linguistic components\n\
	clean_corpus  - remove preprocessed corpus\n\
	clean_character_squeezer_stat - remove generated stat files for character_squeezer\n\
	clean_ngram_stat - gather unigram and bigram statistics from corpus\n\
	clean_unigram_stat - remove files with unigram statistics\n\
	clean_bigram_stat  - remove files with bigram statistics\n\
	clean_sentiment    - remove all temporary and compiled data pertaining to sentiment module\n\
	clean_sentiment_corpus - remove converted sentiment corpus\n\
	clean_sentiment_dict - remove lists with sentiment polarity markers\n\
	clean_topics - remove files created by topics\n\n" >&2

######################
# preprocessed_corpus
corpus: ${PREPROCESSED_CORPUS}

${PREPROCESSED_CORPUS}: ${SRC_CORPUS}
	set -e -o pipefail; \
	character_normalizer $^ | noise_cleaner -n | \
	slang_normalizer | umlaut_restorer | gawk 'NF{gsub(/[[:punct:]]+/, " "); \
	sub(/^[[:blank:]]+/, ""); sub(/[[:blank:]]$$/, ""); \
	gsub(/[[:blank:]][[:blank:]]+/, " "); print tolower($$0)}'  > '$@.tmp' && \
	mv '$@.tmp' '$@'

clean_corpus:
	-rm -f ${PREPROCESSED_CORPUS}

##########################
# character_squeezer_stat
CHAR_SQUEEZER_PICKLE := ${LINGBIN_DIR}/lengthened_stat.pckl

character_squeezer_stat: ${LINGBIN_DIR}/lengthened_stat.pckl | \
		    create_dirs

${CHAR_SQUEEZER_PICKLE}: ${PREPROCESSED_CORPUS}
	set -e ; \
	lengthened_stat $^ > '${@}.tmp' && mv '${@}.tmp' '$@'

clean_character_squeezer_stat: clean_corpus
	-rm -f ${LINGBIN_DIR}/lengthened_stat.pckl

#############
# ngram_stat
ngram_stat: unigram_stat bigram_stat

unigram_stat bigram_stat: %: ${LINGBIN_DIR}/%.pckl

unigram_stat: GRAM_SIZE := 1
bigram_stat:  GRAM_SIZE := 2

${LINGBIN_DIR}/unigram_stat.pckl ${LINGBIN_DIR}/bigram_stat.pckl: ${PREPROCESSED_CORPUS}
	set -e -o pipefail; \
	ngram_stat -n ${GRAM_SIZE} $< > $@.tmp && mv $@.tmp $@

clean_ngram_stat: clean_unigram_stat clean_bigram_stat

clean_unigram_stat clean_bigram_stat: clean_%: clean_corpus
	-rm -f ${LINGBIN_DIR}/$*.pckl

#################################
# topics
# number of topics to be distinguished
N_TOPICS := 40
TOPICS_CORPUS := ${LINGTMP_DIR}/topics_corpus.txt
TOPIC_MODEL_PICKLE = ${LINGBIN_DIR}/topics.%.pckl

topics: topics_bernoulli topics_multinomial

topics_bernoulli topics_multinomial: topics_% : ${TOPIC_MODEL_PICKLE}

${TOPIC_MODEL_PICKLE}: ${TOPICS_CORPUS}
	set -e; \
	topics_train_parameters '--model=$*' --number-of-topics=${N_TOPICS} \
	'$<' > '$@.tmp' && mv '$@.tmp' '$@'

${TOPICS_CORPUS}: ${SRC_CORPUS} ${CHAR_SQUEEZER_PICKLE}
	set -e; \
	topics_train_corpus '$<' > '$@.tmp' && \
	mv '${@}.tmp' '$@'

clean_topics:
	-rm -f '${TOPICS_CORPUS}' ${LINGBIN_DIR}/topics*

############
# sentiment
sentiment: sentiment_corpus sentiment_train sentiment_dict

clean_sentiment: clean_sentiment_corpus \
	clean_sentiment_train \
	clean_sentiment_dict

#################################
# sentiment corpus
SENTIMENT_CORPUS_ROOT := ${SOCMEDIA_LINGSRC}/corpus/sentiment
SENTIMENT_CORPUS_SRC := ${SENTIMENT_CORPUS_ROOT}/twitter.sentiment.xml

# MMAX-related stuff
SENTIMENT_CORPUS_MMAX_ROOT := ${SENTIMENT_CORPUS_ROOT}/mmax-prj
SENTIMENT_CORPUS_MMAX_ORIG := ${SENTIMENT_CORPUS_MMAX_ROOT}/source
SENTIMENT_CORPUS_MMAX_BASE := ${SENTIMENT_CORPUS_MMAX_ROOT}/basedata
SENTIMENT_CORPUS_MMAX_ANNO := ${SENTIMENT_CORPUS_MMAX_ROOT}/markables

SENTIMENT_CORPUS_ORIG_SFX := .xml
SENTIMENT_CORPUS_ORIG_FILES := $(wildcard $(SENTIMENT_CORPUS_MMAX_ORIG)/*$(SENTIMENT_CORPUS_ORIG_SFX))

SENTIMENT_CORPUS_BASE_SFX := .words${SENTIMENT_CORPUS_ORIG_SFX}

# target directory for storing corpus
SENTIMENT_CORPUS_TRGDIR := ${LINGTMP_DIR}/corpus/sentiment

# corpus with CONLL trees
SENTIMENT_CONLL_CORPUS := ${SENTIMENT_CORPUS_TRGDIR}/corpus.sentiment.conll

# directory for storing DB files
SENTIMENT_CORPUS_DBDIR := ${SENTIMENT_CORPUS_TRGDIR}/db
SENTIMENT_CORPUS_DBSFX := .db

# directories for storing training, development, and test corpora
SENTIMENT_CORPUS_TRAINDIR := ${SENTIMENT_CORPUS_TRGDIR}/train
SENTIMENT_CORPUS_DEVTESTDIR := ${SENTIMENT_CORPUS_TRGDIR}/devtest
SENTIMENT_CORPUS_TESTDIR := ${SENTIMENT_CORPUS_TRGDIR}/test

# add directories to the list of automatically created directories
DIR_LIST += ${SENTIMENT_CORPUS_TRAINDIR} ${SENTIMENT_CORPUS_DEVTESTDIR} \
	    ${SENTIMENT_CORPUS_TESTDIR} ${SENTIMENT_CORPUS_DBDIR}

# Split annotated source data into train, devtest, and test part.
# Additionally, for each part, `DESTDIR` variable is set to a local
# value.
SENTIMENT_CORPUS_TRAIN_PART := $(shell get_range -70 $(SENTIMENT_CORPUS_ORIG_FILES))
SENTIMENT_CORPUS_DEVTEST_PART := $(shell get_range 70-85 $(SENTIMENT_CORPUS_ORIG_FILES))
SENTIMENT_CORPUS_TEST_PART := $(shell get_range 85- $(SENTIMENT_CORPUS_ORIG_FILES))

# For each source file in the above parts, we should generate one DB
# file in the db/ directory.  Additionally, we also should extract all
# tweet id's present in the source file and generate exactly one file
# for each tweet in the target directory.  This file, in turn, will
# depend on the previously generated DB file.  Description of these
# tasks is accomplished by a chain of procedures.

# template for generating DB file and rules for single tweet files
define GENERATE_CORPUS_FILES
dbfname := $$(addprefix $(SENTIMENT_CORPUS_DBDIR)/,\
$$(addsuffix $(SENTIMENT_CORPUS_DBSFX),$$(notdir $$(basename $1))))

# extract all tweet id's from __fname and convert them to file names
tmp_flist := $$(addprefix $2/,$$(addsuffix \
$(SENTIMENT_CORPUS_DBSFX),$$(shell get_tweet_ids $1)))

# make the new files in `$(DEST_DIR)` depend on dbfname and specify a
# recipe for their creation
$${tmp_flist}: $${dbfname}
	set -e -o pipefail; \
	get_chunk_by_id --id=$$(basename $$(notdir $$@)) $$< > $$@.tmp && \
	mv $$@.tmp $$@

# add newly generated file names to the specified variable
$3 += ${tmp_flist}
endef

# generate files for the training corpus
SENTIMENT_TRAIN_CORPUS :=
$(foreach fname,$(SENTIMENT_CORPUS_TRAIN_PART),\
$(eval $(call GENERATE_CORPUS_FILES,$(fname),$(SENTIMENT_CORPUS_TRAINDIR),SENTIMENT_TRAIN_CORPUS)))
SENTIMENT_TRAIN_FLIST := ${SENTIMENT_CORPUS_TRAINDIR}/train_file_list

# generate files for the devtest corpus
SENTIMENT_DEVTEST_CORPUS :=
$(foreach fname,$(SENTIMENT_CORPUS_DEVTEST_PART),\
$(eval $(call GENERATE_CORPUS_FILES,$(fname),$(SENTIMENT_CORPUS_DEVTESTDIR),SENTIMENT_DEVTEST_CORPUS)))

# generate files for the test corpus
SENTIMENT_TEST_CORPUS :=
$(foreach fname,$(SENTIMENT_CORPUS_TEST_PART),\
$(eval $(call GENERATE_CORPUS_FILES,$(fname),$(SENTIMENT_CORPUS_TESTDIR),SENTIMENT_TEST_CORPUS)))

# actual corpus rules and dependencies
sentiment_corpus: ${SENTIMENT_TRAIN_FLIST} \
	${SENTIMENT_TRAIN_CORPUS} ${SENTIMENT_DEVTEST_CORPUS} \
	${SENTIMENT_TEST_CORPUS}

# do the clean-up (need find + xargs here due to large number of files)
clean_sentiment_corpus: clean_sentiment_db_corpus
	-rm -f '${SENTIMENT_CONLL_CORPUS}' '${SENTIMENT_TRAIN_FLIST}' && \
	find '${SENTIMENT_CORPUS_TRAINDIR}' '${SENTIMENT_CORPUS_DEVTESTDIR}' \
	'${SENTIMENT_CORPUS_TESTDIR}' -type f -print0 | xargs --null rm -f

# store list of all training files in one place
${SENTIMENT_TRAIN_FLIST}: ${SENTIMENT_TRAIN_CORPUS}
	set -e -o pipefail; \
	find ${<D} -type f -name '*$(suffix $<)' > $@.tmp && mv $@.tmp $@

# extract DG trees from raw unannotated sentiment corpus
${SENTIMENT_CONLL_CORPUS}: ${SENTIMENT_CORPUS_SRC}
	set -e -o pipefail; \
	xml2tsv $< | TextTagger --no-lang-filter | TextParser -t > $@.tmp && \
	mv $@.tmp $@

# DB files used for single tweet files will be built by using an
# implicit rule
${SENTIMENT_CORPUS_DBDIR}/%${SENTIMENT_CORPUS_DBSFX}: \
	${SENTIMENT_CONLL_CORPUS} ${SENTIMENT_CORPUS_MMAX_ORIG}/%${SENTIMENT_CORPUS_ORIG_SFX} \
	${SENTIMENT_CORPUS_MMAX_BASE}/%${SENTIMENT_CORPUS_BASE_SFX} \
	$$(wildcard $(SENTIMENT_CORPUS_MMAX_ANNO)/%_*)
	set -e -o pipefail; \
	merge_conll_mmax $^ | conll2db > $@.tmp && mv $@.tmp $@

# remove database files
clean_sentiment_db_corpus:
	-rm -f $(wildcard $(SENTIMENT_CORPUS_DBDIR)/*$(SENTIMENT_CORPUS_DBSFX))

#################################
# sentiment training
SENTIMENT_RULE_MAIN := ${SOCMEDIA_LINGSRC}/sentiment/main.mln
SENTIMENT_RES_DIR := ${SOCMEDIA_LINGTMP}/sentiment
SENTIMENT_RES := ${SENTIMENT_RES_DIR}/sentiment.mln
NONEVIDENCE_PRED ?= isSentiment,hasSentimentPolarity,isTarget,isSource
CLOSEDWORLD_PRED ?= isEmoexpression,hasEmoexpressionPolarity,hasContextualPolarity,isIntensifier,isDiminisher,isNegation
DIR_LIST += ${SENTIMENT_RES_DIR}

# `train_sentiment' is just an alias for `sentiment_train'
train_sentiment: sentiment_train
sentiment_train: ${SENTIMENT_RES}

${SENTIMENT_RES}: ${SENTIMENT_RULE_MAIN} ${SENTIMENT_TRAIN_FLIST} ${SENTIMENT_TRAIN_CORPUS}
	set -e -o pipefail; \
	learnwts -i $< -o $@.tmp -l '${SENTIMENT_TRAIN_FLIST}' \
	-multipleDatabases -ms -lazy -d -ne '${NONEVIDENCE_PRED}' -cw ${CLOSEDWORLD_PRED} \
	-dMaxMin 30.0 -dLearningRate 1 -memLimit 10240 && mv $@.tmp $@

clean_train_sentiment:
	-rm -f ${SENTIMENT_RES}

#################################
# sentiment testing
sentiment_test:

#######################
# sentiment dictionary
# (macro expansion of SentiWS)
SENTIMENT_DICT_SRCDIR := ${SOCMEDIA_LINGSRC}/sentiment_dict
SENTIMENT_DICT_FILES  := ${LINGTMP_DIR}/negative.txt ${LINGTMP_DIR}/positive.txt

sentiment_dict: create_dirs ${SENTIMENT_DICT_FILES}

# Note, that gawk's functions are locale aware, so setting locale to
# utf-8 will correctly lowercase input letters
${SENTIMENT_DICT_FILES}: ${LINGTMP_DIR}/%.txt: ${SENTIMENT_DICT_SRCDIR}/%.azm \
					      ${SOCMEDIA_LINGSRC}/defines.azm
	set -e -o pipefail; \
	test "$${LANG##*\.}" == "UTF-8" && zoem -i "${<}" -o - | \
	gawk -F "\t" -v OFS="\t" '/^##!/{print; next}; NF {sub(/(^|[[:space:]]+)#.*$$/, "")} \
	$$0 {$$1 = tolower($$1); print}' > "${@}.tmp" && \
	mv "${@}.tmp" "${@}"

clean_sentiment_dict:
	-rm -f ${SENTIMENT_DICT_FILES}
